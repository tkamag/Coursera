{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Classification\n",
    "\n",
    "**Key Concepts**\n",
    "* To underestand different Classification methods.\n",
    "* To apply Classification algorithms on various datasets to solve real world problems.\n",
    "* To underestand evaluation methods in Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1 Introduction to CLassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2 K-Nearest Neighbours\n",
    "The **K-Nearest Neighbors** algorithm is a **classification algorithm that takes a bunch of labeled points and uses them to learn how to label other points**. This algorithm classifies cases based on their similarity to other cases. In **K-Nearest Neighbors**, data points that are near each other are said to be **neighbors**. **K-Nearest Neighbors** is based on this paradigm. \n",
    "> **Similar cases with the same class labels are near each other.** Thus, the distance between two cases is a measure of their dissimilarity.\n",
    "\n",
    "A **low value of K causes a highly complex model as well**, which might result in overfitting of the model. It means **the prediction process is not generalized enough to be used for out-of-sample cases**. \n",
    "> **Out-of-sample** data is data that is outside of the data set used to train the model. In other words, **it cannot be trusted to be used for prediction of unknown samples.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.1  Evaluation Metrics in Classification\n",
    "**Evaluation metrics explain the performance of a model.**\n",
    "> **Precision** is a measure of the accuracy, provided that a class label has been predicted. \n",
    "\n",
    "> **Recall** is the true positive rate.\n",
    "\n",
    "> The **F1 score is the harmonic average of the precision and recall**, where an F1 score reaches its best value at 1 (which represents perfect precision and recall) and its worst at 0. It is a good way to show that a classifier has a good value for both recall and precision. It is defined using the F1-score equation.\n",
    "\n",
    "> **Logarithmic loss (also known as Log loss) measures the performance of a classifier where the predicted output is a probability value between 0 and 1.** \n",
    "\n",
    "We can calculate the **log loss** for each row using the log loss equation, **which measures how far each prediction is, from the actual label**. Then, we calculate the average **log loss** across all rows of the test set. **It is obvious that more ideal classifiers have progressively smaller values of log loss.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3 Introduction to Decision Trees\n",
    "* Each **internal node** corresponds to a test, and \n",
    "* Each **branch** corresponds to a result of the test, and \n",
    "* Each **leaf** node assigns a patient to a class. \n",
    "\n",
    "**Entropy** is the amount of information disorder or the amount of randomness in the data. The entropy in the node depends on how much random data is in that node and is calculated for each node. \n",
    "\n",
    "In decision trees, **we're looking for trees that have the smallest entropy in their nodes**. The **entropy** is used to **calculate the homogeneity of the samples in that node**. \n",
    "* If the samples are **completely homogeneous**, the **entropy** is zero and \n",
    "* If the samples are **equally divided it has an entropy of one**. This means if all the data in a node **are either drug A or drug B**, then the **entropy is zero**, but if half of the data or drug A and other half or B then the **entropy is one**. You can easily calculate the entropy of a node using the frequency table of the attribute through the entropy formula\n",
    "\n",
    "**Information gain** is the information that can **increase the level of certainty after splitting**. It is the **entropy of a tree before the split minus the weighted entropy after the split by an attribute.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4 Intro to Logistic Regression\n",
    "**Logistic regression** is a statistical and machine learning technique for classifying records of a dataset based on the values of the input fields.\n",
    "\n",
    "**Logistic regression** is analogous to **linear regression**, but tries to predict a categorical or discrete target field instead of a numeric one. In **linear regression**, we might try to predict a continuous value of variables such as the price of a house, blood pressure of a patient, or fuel consumption of a car. But in **logistic regression**, we predict a variable which is binary such as yes/no, true/false, successful or not successful, pregnant/not pregnant, and so on, all of which can be coded as zero or one. \n",
    "\n",
    "> In **logistic regression**, we **model the probability that an input, x, belongs to the default class y equals 1**, and we can write this formally as probability of y equals 1 given x.\n",
    "**P(Y=1|X)**\n",
    "\n",
    "\n",
    "**For example**, the probability of a customer staying with the company can be shown as probability of churn equals 1 given a customer's income and age, **P(Churn=1|income, age)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.5 Support Vector Machine\n",
    "A **Support Vector Machine** is a supervised algorithm **that can classify cases by finding a separator**. SVM works by \n",
    "* **First** mapping data to a high dimensional feature space so that **data points can be categorized, even when the data are not otherwise linearly separable**. \n",
    "* Then, **a separator is estimated for the data**. The data should be transformed in such a way that a separator could be drawn as a hyperplane. \n",
    "\n",
    "Basically, mapping data into a higher-dimensional space is called, **kernelling**. The mathematical function used for the transformation is known as the kernel function, and can be of different types, such as **linear, polynomial, Radial Basis Function, or RBF, and sigmoid**. Each of these functions has its own characteristics,\n",
    "> **the goal is to choose a hyperplane with as big a margin as possible.**\n",
    "\n",
    "The **disadvantages of Support Vector Machines** include the fact that \n",
    "* **the algorithm is prone for over-fitting if the number of features is much greater than the number of samples**. \n",
    "* Also, **SVMs do not directly provide probability estimates**, which are desirable in most classification problems. \n",
    "* And finally, **SVMs are not very efficient computationally if your dataset is very big**, such as when you have more than 1,000 rows.\n",
    "\n",
    "Well, \n",
    "* **SVM** is good for image analysis tasks, such as **image classification** and **hand written digit recognition**.\n",
    "\n",
    "* **SVM** is very effective in **text mining tasks**, particularly due to its effectiveness in dealing with high-dimensional data. **For example**, it is used for detecting spam, text category assignment and sentiment analysis.\n",
    "\n",
    "* Another application of **SVM** is in **gene expression data classification**, again, because of **its power in high-dimensional data classification**. \n",
    "\n",
    "* **SVM** can also be used for other types of machine learning problems, such as **regression, outlier detection and clustering**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
