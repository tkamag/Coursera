{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Classification\n",
    "\n",
    "**Key Concepts**\n",
    "* To underestand different Classification methods.\n",
    "* To apply Classification algorithms on various datasets to solve real world problems.\n",
    "* To underestand evaluation methods in Classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1 Introduction to CLassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2 K-Nearest Neighbours\n",
    "The **K-Nearest Neighbors** algorithm is a **classification algorithm that takes a bunch of labeled points and uses them to learn how to label other points**. This algorithm classifies cases based on their similarity to other cases. In **K-Nearest Neighbors**, data points that are near each other are said to be **neighbors**. **K-Nearest Neighbors** is based on this paradigm. \n",
    "> **Similar cases with the same class labels are near each other.** Thus, the distance between two cases is a measure of their dissimilarity.\n",
    "\n",
    "A **low value of K causes a highly complex model as well**, which might result in overfitting of the model. It means **the prediction process is not generalized enough to be used for out-of-sample cases**. \n",
    "> **Out-of-sample** data is data that is outside of the data set used to train the model. In other words, **it cannot be trusted to be used for prediction of unknown samples.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.1  Evaluation Metrics in Classification\n",
    "**Evaluation metrics explain the performance of a model.**\n",
    "> **Precision** is a measure of the accuracy, provided that a class label has been predicted. \n",
    "\n",
    "> **Recall** is the true positive rate.\n",
    "\n",
    "> The **F1 score is the harmonic average of the precision and recall**, where an F1 score reaches its best value at 1 (which represents perfect precision and recall) and its worst at 0. It is a good way to show that a classifier has a good value for both recall and precision. It is defined using the F1-score equation.\n",
    "\n",
    "> **Logarithmic loss (also known as Log loss) measures the performance of a classifier where the predicted output is a probability value between 0 and 1.** \n",
    "\n",
    "We can calculate the **log loss** for each row using the log loss equation, **which measures how far each prediction is, from the actual label**. Then, we calculate the average **log loss** across all rows of the test set. **It is obvious that more ideal classifiers have progressively smaller values of log loss.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3 Introduction to Decision Trees\n",
    "* Each **internal node** corresponds to a test, and \n",
    "* Each **branch** corresponds to a result of the test, and \n",
    "* Each **leaf** node assigns a patient to a class. \n",
    "\n",
    "**Entropy** is the amount of information disorder or the amount of randomness in the data. The entropy in the node depends on how much random data is in that node and is calculated for each node. \n",
    "\n",
    "In decision trees, **we're looking for trees that have the smallest entropy in their nodes**. The **entropy** is used to **calculate the homogeneity of the samples in that node**. \n",
    "* If the samples are **completely homogeneous**, the **entropy** is zero and \n",
    "* If the samples are **equally divided it has an entropy of one**. This means if all the data in a node **are either drug A or drug B**, then the **entropy is zero**, but if half of the data or drug A and other half or B then the **entropy is one**. You can easily calculate the entropy of a node using the frequency table of the attribute through the entropy formula\n",
    "\n",
    "**Information gain** is the information that can **increase the level of certainty after splitting**. It is the **entropy of a tree before the split minus the weighted entropy after the split by an attribute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
