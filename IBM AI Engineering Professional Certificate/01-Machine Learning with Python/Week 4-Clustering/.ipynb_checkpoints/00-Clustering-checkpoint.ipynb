{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Clustering\n",
    "**Key Concepts**\n",
    "* To underestand different types of clustering algorithms.\n",
    "* To apply clustering on different types of datasests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1 k-Means Clustering\n",
    "**Customer segmentation** is the practice of partitioning a customer base into groups of individuals that have similar characteristics. It is a significant strategy, as it allows the business to target specific groups of customers, so as to more effectively allocate marketing resources.\n",
    "\n",
    "Customers can be grouped based on several factors, including; age, gender, interests, spending habits and so on. The important requirement is to use the available data to understand and identify how customers are similar to each other. \n",
    "\n",
    "One of the most adopted approaches that can be used for customer segmentation is **clustering**. **Clustering** can group data only unsupervised, based on the similarity of customers to each other. It will partition your customers into mutually exclusive groups. \n",
    "\n",
    "**Clustering** means finding **clusters in a dataset, unsupervised**. So what is a cluster? **A cluster is a group of data points or objects in a dataset that are similar to other objects in the group, and dissimilar to datapoints in other clusters.**\n",
    "\n",
    "We can use a clustering algorithm such as **k-means** to **group similar customers as mentioned, and assign them to a cluster, based on whether they share similar attributes, such as; age, education, and so on.**\n",
    "\n",
    "In the **retail industry**, clustering is **used to find associations among customers based on their demographic characteristics and use that information to identify buying patterns of various customer groups.**\n",
    "\n",
    "It can be used in **recommendation systems** to **find a group of similar items or similar users and use it for collaborative filtering, to recommend things like books or movies to customers.**\n",
    "\n",
    "In banking, **analysts find clusters of normal transactions to find the patterns of fraudulent credit card usage**. Also they use clustering to identify clusters of customers. \n",
    "\n",
    "\n",
    "For instance, **to find loyal customers versus churned customers**. In the insurance industry, clustering is used for fraud detection in claims analysis, or to evaluate the insurance risk of certain customers based on their segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.2 Intro to k-Means\n",
    "**K-Means** is a type of **partitioning clustering**, that is, **it divides the data into $K$ non-overlapping subsets or clusters without any cluster internal structure or labels**. This means, it's an unsupervised algorithm. \n",
    "> **We can say K-Means tries to minimize the intra-cluster distances and maximize the inter-cluster distances.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.3 More on k-Means\n",
    "\n",
    "> **Average of the distances of data points from their cluster centroids can be used as a metric of error for the clustering algorithm**.\n",
    "\n",
    "**k-Means** is **partition-based clustering** which is a, \n",
    "* relatively efficient on **medium and large sized data sets**. \n",
    "* produces **sphere-like clusters** because the clusters are shaped around the centroids.\n",
    "* its **drawback is that we should pre-specify the number of clusters**, and this is not an easy task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.4 Hierarchical Clustering\n",
    "**Hierarchical clustering** algorithms **build a hierarchy of clusters where each node is a cluster consisting of the clusters of its daughter nodes**. Strategies for hierarchical clustering generally fall into two types, **divisive and agglomerative**. \n",
    "\n",
    "* **Divisive** is top down, so you start with all observations in a large cluster and break it down into smaller pieces*. **Think about divisive as dividing the cluster*. \n",
    "\n",
    "* **Agglomerative** is the opposite of divisive. So it is bottom up, where each observation starts in its own cluster and pairs of clusters are merged together as they move up the hierarchy. **Agglomeration means to amass or collect things**, which is exactly what this does with the cluster.\n",
    "\n",
    "**Hierarchical clustering** is typically visualized as a **dendrogram**. **Each merge is represented by a horizontal line**. The y-coordinate of the horizontal line is **the similarity of the two clusters that were merged where cities are viewed as singleton clusters**. By moving up from the bottom layer to the top node, **a dendrogram allows us to reconstruct the history of merges that resulted in the depicted clustering**. \n",
    "\n",
    "Essentially, **hierarchical clustering does not require a prespecified number of clusters**. However, in some applications, we want a partition of disjoint clusters just as in flat clustering. In those cases, the hierarchy needs to be cut at some point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.5 More on Hierarchical Clustering\n",
    "Let's get started. Let's look at agglomerative algorithm for hierarchical clustering. Let's say our data set has **n data points**. \n",
    "* **First**, we want to create **n clusters**, one for each data point. Then, each point is assigned as a cluster.\n",
    "\n",
    "* **Next**, we want **to compute the distance proximity matrix** which will be an n by n table. After that, we want to iteratively run the following steps until the specified cluster number is reached, or until there is only one cluster left. \n",
    "    * **First*, merge the **two nearest clusters**. Distances are computed already in the proximity matrix. \n",
    "    * **Second**, update **the proximity matrix with the new values**. We stop after we've reached the specified number of clusters, or there is only one cluster remaining with the result stored in a **dendogram**.\n",
    "> So in the **proximity matrix, we have to measure the distances between clusters and also merge the clusters that are nearest**. The key operation is the computation of the proximity between the clusters with one point and also clusters with multiple data points.\n",
    "\n",
    "We can use different distance measurements to calculate the proximity matrix. For instance, Euclidean distance. So, if we have a data set of n patience, we can billed an n by n dissimilarity distance matrix. It will give us the distance of clusters with one data point. However, as mentioned, we merge clusters in agglomerative clustering. Now the question is, how can we calculate the distance between clusters when there are multiple patients in each cluster?\n",
    "\n",
    "We can use different criteria to find the closest clusters and merge them. In general, it completely depends on the data type, dimensionality of data and most importantly, the domain knowledge of the data set. In fact, different approaches to defining the distance between clusters distinguish the different algorithms. As you might imagine, there are multiple ways we can do this. \n",
    "* The first one is called **single linkage clustering**. **Single linkage** is defined as **the shortest distance between two points in each cluster, such as point a and b**.\n",
    "\n",
    "* Next up is **complete linkage clustering**. This time we are finding **the longest distance between the points in each cluster, such as the distance between point a and b**.\n",
    "\n",
    "* The third type of linkage is **average linkage clustering or the mean distance**. This means we're looking at **the average distance of each point from one cluster to every point in another cluster.**\n",
    "\n",
    "* The final linkage type to be reviewed is **centroid linkage clustering**. Centroid is **the average of the feature sets of points in a cluster**. This linkage **takes into account the centroid of each cluster when determining the minimum distance**.\n",
    "\n",
    "There are **three main advantages** to using hierarchical clustering. \n",
    "* **First**, we **do not need to specify the number of clusters** required for the algorithm. \n",
    "* **Second**, hierarchical clustering **is easy to implement**. \n",
    "* Third, the **dendrogram produced** is very useful in understanding the data.\n",
    "\n",
    "There are **some disadvantages** as well. \n",
    "* **First**, the algorithm **can never undo** any previous steps. So for example, the algorithm clusters two points and later on, we see that the connection was not a good one. The program can not undo that step.\n",
    "* **Second**, the **time complexity for the clustering can result in very long computation times in comparison with efficient algorithms such as K-means**. \n",
    "* **Finally**, if we have a large data set, **it can become difficult to determine the correct number of clusters by the dendrogram.**\n",
    "\n",
    "> **K-means** is more efficient for **large data sets**. In contrast to **K-means**, **hierarchical clustering** does not **require the number of cluster to be specified**. \n",
    "\n",
    "> **Hierarchical clustering** gives **more than one partitioning** depending on the resolution or as **K-means** gives **only one partitioning** of the data. \n",
    "\n",
    "> **Hierarchical clustering** always **generates the same clusters**, in contrast with **K-means**, that **returns different clusters** each time it is run, **due to random initialization of centroids.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.6 Density Based Clustering\n",
    "A **density-based clustering** algorithm is appropriate to use when examining spatial data. Most of the traditional clustering techniques such as **K-Means, hierarchical, and Fuzzy clustering** can be used **to group data in an unsupervised way**. However, **when applied to tasks with arbitrary shaped clusters or clusters within clusters**, traditional techniques might not be able to achieve good results that is, elements in the same cluster might not share enough similarity or the performance may be poor.\n",
    "> Additionally, **while partitioning based algorithms such as K-Means may be easy to understand and implement in practice, the algorithm has no notion of outliers.** all points are assigned to a cluster even if they do not belong in any. In the domain of anomaly detection, this causes problems as anomalous points will be assigned to the same cluster as normal data points.  \n",
    "\n",
    "In contrast, **density-based clustering** locates regions of high density that are separated from one another by regions of low density. **Density** in this context is defined **as the number of points within a specified radius**. A specific and very popular type of density-based clustering is **DBSCAN**. \n",
    "> **DBSCAN is particularly effective for tasks like class identification on a spatial context.** The wonderful attributes of the **DBSCAN** algorithm is that it can find out any arbitrary shaped cluster without getting effected by noise. \n",
    "> **DBSCAN** can be used here to find the group which show the same weather condition. It not only finds **different arbitrary shaped clusters** it can find the **denser part of data-centered samples by ignoring less dense areas or noises**.\n",
    "\n",
    "> **DBSCAN stands for Density-Based Spatial Clustering of Applications with Noise.** This technique is one of the most common clustering algorithms which works based on density of object.\n",
    "\n",
    "**DBSCAN** works on the idea that **if a particular point belongs to a cluster it should be near to lots of other points in that cluster**. It works based on two parameters; **radius and minimum points.**\n",
    "* $R$ determines a **specified radius** that if it includes enough points within it, **we call it a dense area**. \n",
    "* $M$ determines the **minimum number of data points we want in a neighborhood to define a cluster**. \n",
    "\n",
    "To see how **DBSCAN** works, we have to determine **the type of points**. Each point in our dataset can be either **a core, border, or outlier point**.\n",
    "> But the whole idea behind the **DBSCAN algorithm is to visit each point and find its type first, then we group points as clusters based on their types**. \n",
    "\n",
    "Let's pick a point randomly. \n",
    "* **First**, we check to see whether it's a core data point. So, what is a core point? \n",
    "    * A **data point is a core point if within our neighborhood of the point there are at least M points.**  \n",
    "    * A **data point is a border point if A; its neighbourhood contains less than M data points or B; it is reachable from some core point**. Here, reachability means **it is within our distance from a core point.**\n",
    "    * An **outlier is a point that is not a core point and also is not close enough to be reachable from a core point.**\n",
    "\n",
    "* The next step is **to connect core points that are neighbors and put them in the same cluster.** So, a **cluster** is formed as at least one core point plus all reachable core points plus all their borders. It's simply shapes all the clusters and finds outliers as well. \n",
    "\n",
    "Let's review this one more time to see why **DBSCAN is cool**. \n",
    "* **DBSCAN** can **find arbitrarily shaped clusters**. It can even **find a cluster completely surrounded by a different cluster**. \n",
    "\n",
    "* **DBSCAN** has a notion **of noise and is robust to outliers**. On top of that, \n",
    "\n",
    "* **DBSCAN makes it very practical for use in many real-world problems** because it **does not require one to specify the number of clusters such as K in K-means.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
